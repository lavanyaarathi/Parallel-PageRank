# PageRank Methods Overview

A visual guide to understanding the different PageRank calculation methods.

## ğŸ¯ Three Ways to Calculate PageRank

All three methods implement the **same PageRank algorithm**, but use different parallelization strategies:

```
PR(v) = (1-d)/N + d Ã— Î£(PR(u)/L(u))
```

The difference is **HOW** they compute it.

---

## ğŸ“Š Method Comparison at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PAGERANK IMPLEMENTATIONS                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SERIAL     â”‚  â”‚  PTHREADS    â”‚  â”‚     MPI      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  [Node 1]    â”‚  â”‚ Thread 1:    â”‚  â”‚ Process 1:   â”‚
â”‚  [Node 2]    â”‚  â”‚ [Node 1-2]   â”‚  â”‚ [Node 1-2]   â”‚
â”‚  [Node 3]    â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  [Node 4]    â”‚  â”‚ Thread 2:    â”‚  â”‚ Process 2:   â”‚
â”‚              â”‚  â”‚ [Node 3-4]   â”‚  â”‚ [Node 3-4]   â”‚
â”‚  Sequential  â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  Processing  â”‚  â”‚ Parallel     â”‚  â”‚ Distributed  â”‚
â”‚              â”‚  â”‚ (Shared Mem) â”‚  â”‚ (Network)    â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  1 Core      â”‚  â”‚ 4-8 Cores    â”‚  â”‚ Many Nodes   â”‚
â”‚  1x Speed    â”‚  â”‚ 2-8x Speed   â”‚  â”‚ 4-100x Speed â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Execution Flow Comparison

### Serial Method
```
Start
  â†“
Read Graph
  â†“
Initialize PR values
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each node:  â”‚ â† Sequential loop
â”‚  Compute PR     â”‚
â”‚  Update values  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Check convergence
  â†“
[Not converged?] â†’ Yes â†’ Loop back
  â†“ No
Output results
  â†“
End
```

### Pthreads Method
```
Start
  â†“
Read Graph
  â†“
Initialize PR values
  â†“
Create N threads
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thread 1: Process nodes 1-2    â”‚
â”‚ Thread 2: Process nodes 3-4    â”‚ â† Parallel execution
â”‚ Thread 3: Process nodes 5-6    â”‚
â”‚ Thread 4: Process nodes 7-8    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Synchronize (mutex locks)
  â†“
Check convergence
  â†“
[Not converged?] â†’ Yes â†’ Loop back
  â†“ No
Output results
  â†“
End
```

### MPI Method
```
Start (on each process)
  â†“
Process 0: Read Graph â†’ Convert to CSR
  â†“
Broadcast graph to all processes
  â†“
Each process gets partition
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Process 1: Process nodes 1-2   â”‚
â”‚ Process 2: Process nodes 3-4   â”‚ â† Distributed execution
â”‚ Process 3: Process nodes 5-6   â”‚
â”‚ Process 4: Process nodes 7-8   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
MPI_Allgatherv (exchange values)
  â†“
MPI_Allreduce (compute global sums)
  â†“
Check convergence
  â†“
[Not converged?] â†’ Yes â†’ Loop back
  â†“ No
Output results
  â†“
End
```

---

## ğŸ’¾ Memory Model Comparison

### Serial
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Single Memory Space       â”‚
â”‚                             â”‚
â”‚  [All Nodes]                â”‚
â”‚  [All Edges]                â”‚
â”‚  [All PR values]            â”‚
â”‚                             â”‚
â”‚  One process accesses all   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pthreads
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Shared Memory Space       â”‚
â”‚                             â”‚
â”‚  [All Nodes] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  [All Edges]            â”‚   â”‚
â”‚  [All PR values]        â”‚   â”‚
â”‚                         â”‚   â”‚
â”‚  Thread 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  Thread 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  Thread 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  Thread 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚
â”‚  All threads share memory   â”‚
â”‚  (use mutex for safety)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MPI
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Process 1   â”‚  â”‚  Process 2   â”‚  â”‚  Process 3   â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ [Nodes 1-2]  â”‚  â”‚ [Nodes 3-4]  â”‚  â”‚ [Nodes 5-6]  â”‚
â”‚ [Edges 1-2]  â”‚  â”‚ [Edges 3-4]  â”‚  â”‚ [Edges 5-6]  â”‚
â”‚ [PR 1-2]     â”‚  â”‚ [PR 3-4]     â”‚  â”‚ [PR 5-6]     â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              Network Communication
              (MPI messages)
```

---

## âš¡ Performance Characteristics

### Speedup Comparison

```
Speedup
  â†‘
  â”‚                                    MPI (ideal)
  â”‚                                  â•±
  â”‚                                â•±
  â”‚                              â•±
  â”‚                            â•±
  â”‚                          â•±
  â”‚                        â•±
  â”‚                      â•±
  â”‚                    â•±
  â”‚                  â•±
  â”‚                â•±
  â”‚              â•±
  â”‚            â•±
  â”‚          â•±
  â”‚        â•±
  â”‚      â•±  Pthreads (typical)
  â”‚    â•±
  â”‚  â•±
  â”‚â•±
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Processors
  1    2    4    8   16   32   64
```

### When to Use Each Method

```
Graph Size          Recommended Method
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
< 100K nodes    â†’   Serial
100K - 10M      â†’   Pthreads
> 10M nodes     â†’   MPI
```

---

## ğŸ”§ Technical Differences

### Data Structures

| Method | Graph Storage | PR Storage | Communication |
|--------|--------------|------------|---------------|
| Serial | Adjacency lists | Array | None |
| Pthreads | Adjacency lists (bidirectional) | Array (shared) | Mutex locks |
| MPI | CSR format | Distributed arrays | MPI messages |

### Synchronization

| Method | Sync Mechanism | Overhead |
|--------|---------------|----------|
| Serial | None | None |
| Pthreads | Mutex locks, barriers | Low |
| MPI | MPI_Allgatherv, MPI_Allreduce | Medium-High |

### Scalability

| Method | Max Processors | Bottleneck |
|--------|---------------|------------|
| Serial | 1 | CPU |
| Pthreads | CPU cores (4-64) | Memory bandwidth |
| MPI | Cluster size (100s-1000s) | Network latency |

---

## ğŸ“ˆ Example: Same Problem, Different Approaches

**Problem**: Calculate PageRank for 1,000,000 nodes

### Serial Approach
```
Time: 100 seconds
Memory: 80 MB
Processors: 1
```

### Pthreads Approach
```
Time: 15 seconds (6.7x speedup)
Memory: 96 MB
Processors: 8 cores
```

### MPI Approach
```
Time: 2 seconds (50x speedup)
Memory: 20 MB per process (160 MB total for 8 processes)
Processors: 8 nodes (64 cores total)
```

---

## ğŸ“ Key Takeaways

1. **Same Algorithm**: All three implement identical PageRank formula
2. **Different Execution**: How they parallelize differs
3. **Trade-offs**: 
   - Serial: Simple but slow
   - Pthreads: Good speedup, single machine
   - MPI: Best speedup, requires cluster
4. **Choose Based On**:
   - Graph size
   - Available hardware
   - Performance requirements

---

## ğŸš€ Quick Start

**See the differences in action:**

```bash
# Compare all three methods
python compare_methods.py pagerank_mpi/small_graph.txt 4 0.0001 0.85 4
```

**Learn more:**
- [QUICKSTART.md](QUICKSTART.md) - How to run the project
- [COMPARISON.md](COMPARISON.md) - Detailed technical comparison
- [FEATURES.md](FEATURES.md) - All features and optimizations

